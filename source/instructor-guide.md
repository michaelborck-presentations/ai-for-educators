# AI for Educators: Facilitator Guide

## Session Overview

**Title:** AI for Educators: Understanding Instructions at the Heart of AI
**Duration:** 60 minutes
**Format:** Interactive session with live demos
**Audience:** Faculty and teaching staff
**Delivery:** In-person or online

---

## Pre-Session Checklist

- [ ] Browser open with Gemini (gemini.google.com) or Claude (claude.ai)
- [ ] Companion website loaded (share link at start)
- [ ] Slides ready (share screen if online)
- [ ] This guide open for reference
- [ ] Test screen sharing before session (if online)

---

## Session Flow Summary

| Time | Section | Duration |
|------|---------|----------|
| 0:00 | Opening Hook (Deloitte Case) | 3 min |
| 0:03 | Part 1: The Real Barrier | 10 min |
| 0:13 | Part 2: The Core Concept | 7 min |
| 0:20 | Part 3: Three Techniques | 20 min |
| 0:40 | Part 4: Assessment Reality | 10 min |
| 0:50 | Part 5: Ethics & Next Steps | 8 min |
| 0:58 | Wrap-up & Resources | 2 min |

---

## Opening Hook: The Deloitte Case (3 minutes)

### Slide 1: Title Slide
*Display while being introduced*

### Slide 2: "$440,000"

**Script:**

> "$440,000."
>
> *[Pause 2 seconds]*
>
> "That's what the Australian government paid Deloitte for a welfare review report earlier this year. And $1.6 million for a Canadian healthcare report."
>
> *[Pause]*
>
> "Both reports contained fabricated citations. Fake studies. Made-up quotes. The AI Deloitte used to draft these reports hallucinated - it invented sources that didn't exist. And their consultants never caught it."
>
> *[Pause]*
>
> "Why does this matter for us? Because your students will encounter this. Some already are. Today, I'm going to show you how to think about AI - and how to teach others to use it wisely."

**Transition:** "But before we talk about how to use AI, let's talk about something that doesn't get discussed enough."

---

## Part 1: The Real Barrier (10 minutes)

### Key Message: AI Shame is Real

**Script:**

> "Here's something most AI training doesn't address: AI shame."
>
> "The statistics are striking: 70% of knowledge workers use AI weekly. But 63% fear being seen as incompetent or lazy for using it. Most hide their AI use from colleagues."
>
> *[Pause for recognition]*
>
> "Sound familiar? This creates 'underground use' - people using AI without sharing best practices, without developing skills openly, without learning from each other."
>
> "The reframe: Using AI thoughtfully is a professional skill. Hiding it prevents growth."

**Discussion Prompt (optional):**
> "Quick show of hands or chat: How many of you have used AI for work but not really told anyone? You're not alone."

### Your Expertise Matters MORE

**Script:**

> "Let's address the fear many educators have: 'Will AI make me redundant?'"
>
> "The reality is the opposite. Think about calculators. Did they make maths teachers redundant? No - they changed WHAT we teach and HOW."
>
> "Same with AI. Your expertise in pedagogy, in your discipline, in understanding your students - that becomes MORE valuable, not less. Students don't need you less; they need you MORE to guide them through this shift."

**Transition:** "Now let's talk about how AI actually works - and it's simpler than you might think."

---

## Part 2: The Core Concept (7 minutes)

### Key Message: Instructions are Everything

**Script:**

> "Here's the key insight I want you to take away today:"
>
> "At the heart of ALL AI are instructions. We sometimes call this 'prompt engineering' or 'context engineering' - but really, it's just giving clear instructions."
>
> "The better your instructions, the better the output. That's it."

### The Intern Analogy

**Script:**

> "Think of AI like a brilliant intern. They're eager, fast, and can do amazing work. But they might confidently present completely made-up statistics because they want to impress you."
>
> "Your job is to be their editor-in-chief, not their stenographer."
>
> "The golden rule: AI should challenge your thinking, not replace it. The moment you stop questioning AI's output is the moment you become Deloitte."

**Transition:** "Now let's look at three practical techniques you can use yourself - and teach to others."

---

## Part 3: Three Techniques (20 minutes)

### Technique 1: Board of Directors Strategy (7 minutes)

**Concept Explanation:**

> "When you're making a decision, you want multiple perspectives. In a real organisation, you'd have different stakeholders - each with different concerns."
>
> "We can use AI to simulate this. One prompt, multiple expert perspectives."

**Demo Prompt:**

```
I'm redesigning my assessment for a second-year management unit.

Act as my advisory board and analyse this from multiple perspectives:

1. As Learning Designer: Alignment with learning outcomes?
2. As Academic Integrity Officer: AI vulnerability and integrity?
3. As Student Advocate: Workload, clarity, and fairness?
4. As Industry Partner: Real-world relevance and skills?

For each perspective, give me 3-4 key considerations.
```

**Run Demo:**
- Paste prompt into Gemini/Claude
- Highlight key insights from each "director"
- Note how it catches blind spots

**Participant Try (if time):**
> "Think about something you're working on - an assessment, a policy, a new unit. Try this prompt structure."

---

### Technique 2: Devil's Advocate (6 minutes)

**Concept Explanation:**

> "Confirmation bias is real. When we have an idea, we tend to look for evidence that supports it. The Devil's Advocate technique forces AI to argue against your position."

**Demo Prompt:**

```
I believe that banning AI from assessments is the best approach for maintaining academic integrity.

Play devil's advocate. Give me the strongest possible arguments against this position.
Be harsh but fair. I want to know:

1. Why might this approach fail?
2. What am I probably not thinking about?
3. Who would this NOT work for?
4. What unintended consequences could occur?

Don't hold back - I need to hear the hard truths.
```

**Run Demo:**
- Paste prompt
- Point out how AI identifies blind spots
- Note: "be harsh but fair" gives permission to be critical

**Key Point:**

> "You can use this for ANY position you hold. It's about stress-testing your thinking, not being wrong."

---

### Technique 3: Reverse Prompting (7 minutes)

**Concept Explanation:**

> "Sometimes the hardest part isn't getting answers - it's knowing what to ask. Reverse prompting flips the script: AI interviews YOU first."
>
> "This is powerful when you're starting something new and don't know what you don't know."

**Demo Prompt:**

```
I want to create a policy for AI use in my business management units.

Before giving me any advice, interview me first. Ask me questions one at a time to understand:
- My teaching context and student level
- My current assessment types
- My concerns about AI use
- My goals for student learning
- Any constraints I'm working with

After you understand my situation, then give me tailored recommendations.

Start with your first question.
```

**Run Demo:**
- Paste prompt
- Answer 2-3 of AI's questions
- Show how responses become tailored to YOUR situation

**Key Point:**

> "Generic prompts get generic answers. When AI understands your context, it gives you actionable insights."

**Transition:** "Now let's talk about the elephant in the room for educators: assessments."

---

## Part 4: Assessment Reality (10 minutes)

### The Stress-Test Concept

**Script:**

> "Here's a simple test: Run your assessment through AI."
>
> "If AI can complete it independently and get a pass, the assessment reveals what was already breakable."
>
> "This isn't about banning AI. It's about designing better assessments."

**Three Redesign Options:**

1. **Option A: Process Documentation**
   - Submit AI conversation transcript
   - Annotate where AI was wrong or limited
   - Reflection on how human expertise improved output

2. **Option B: In-Class Components**
   - AI-generated draft submitted before class
   - In-class critique and improvement
   - Live presentation of key insights

3. **Option C: Personal Application**
   - Connect to specific local context
   - Include primary data students collected
   - Require unique perspective AI can't have

**Discussion Prompt:**

> "Quick question: Has anyone already run an assessment through AI to see what happens? What did you find?"

**Key Message:**

> "AI doesn't break assessments; it reveals what was already breakable. This is an opportunity to design better assessments, not a disaster."

**Transition:** "Before we wrap up, ethics and next steps."

---

## Part 5: Ethics & Next Steps (8 minutes)

### The Billboard Rule

**Script:**

> "Before sharing anything with AI, ask: Would I be comfortable if this appeared on a billboard?"
>
> "Never share student data, personal information about others, or confidential institutional data."
>
> "This applies to us AND to our students. Model the behaviour you want to see."

### Three Verification Questions

1. Can I find this source independently?
2. Does this statistic appear elsewhere?
3. Does this make logical sense?

**Script:**

> "If you can't verify it, don't use it. Teach your students these same questions."

### Your One Commitment

**Script:**

> "Before you leave, I want you to think about one small experiment you'll try this week."
>
> "Maybe you'll:
> - Use AI to draft feedback on student work, then edit it
> - Try the Board of Directors prompt on something you're planning
> - Run one of your assessments through AI to see what happens"
>
> "Start small. Scale what works. Share with colleagues."

---

## Wrap-up & Resources (2 minutes)

### Key Takeaways

1. AI shame is real - normalize thoughtful use
2. Instructions are everything - clarity drives quality
3. Use multi-perspective prompting (Board of Directors)
4. Challenge your ideas (Devil's Advocate)
5. Let AI interview you for better context (Reverse Prompting)
6. Always verify - never trust blindly

### Share Resources

- Companion website URL (display QR code if available)
- Educator handout with all prompts
- Diagnostic quiz for personalised learning paths

### Closing

> "The educators who will thrive aren't the ones who avoid AI or blindly embrace it. They're the ones who develop judgment about when and how to use it - and who model that for their students."
>
> "The tools we covered today will serve you in your teaching and help you guide others. Use them wisely."

---

## Appendix A: All Demo Prompts

### Prompt 1: Board of Directors

```
I'm redesigning my assessment for a second-year management unit.

Act as my advisory board and analyse this from multiple perspectives:

1. As Learning Designer: Alignment with learning outcomes?
2. As Academic Integrity Officer: AI vulnerability and integrity?
3. As Student Advocate: Workload, clarity, and fairness?
4. As Industry Partner: Real-world relevance and skills?

For each perspective, give me 3-4 key considerations.
```

### Prompt 2: Devil's Advocate

```
I believe that banning AI from assessments is the best approach for maintaining academic integrity.

Play devil's advocate. Give me the strongest possible arguments against this position.
Be harsh but fair. I want to know:

1. Why might this approach fail?
2. What am I probably not thinking about?
3. Who would this NOT work for?
4. What unintended consequences could occur?

Don't hold back - I need to hear the hard truths.
```

### Prompt 3: Reverse Prompting

```
I want to create a policy for AI use in my business management units.

Before giving me any advice, interview me first. Ask me questions one at a time to understand:
- My teaching context and student level
- My current assessment types
- My concerns about AI use
- My goals for student learning
- Any constraints I'm working with

After you understand my situation, then give me tailored recommendations.

Start with your first question.
```

### Prompt 4: Assessment Stress-Test

```
Here is an assessment task I give to students:

[PASTE YOUR ASSESSMENT BRIEF HERE]

Please complete this assessment as a student would, using only the information
in the brief and your general knowledge.

After completing it, tell me:
1. How well could AI complete this independently?
2. What aspects require genuine student thinking?
3. How might I redesign this to make student judgment more visible?
```

---

## Appendix B: Discussion Questions (if time permits)

1. "Has anyone had AI give them confidently wrong information? What happened?"
2. "What aspects of teaching do you think AI should NOT be used for?"
3. "How are you currently communicating AI expectations to students?"

---

## Appendix C: Troubleshooting

**If AI tool is slow or down:**
- Have screenshots of expected outputs ready
- Switch to backup tool (Gemini, Claude, or ChatGPT)

**If participants can't access AI tools:**
- They can follow along with your demo
- Direct them to companion website for prompts to try later

**If running over time:**
- Shorten participant activities
- Condense assessment section to key points only
- Skip Reverse Prompting demo if needed

---

## Appendix D: Adjusting for Audience

**If audience seems sceptical:**
- Spend more time on AI shame and fear
- Emphasise "your expertise matters MORE"
- Use more discussion time

**If audience seems experienced:**
- Move faster through basics
- Focus on assessment redesign
- Add more advanced techniques from companion website

**If audience has specific concerns:**
- Adapt examples to their discipline
- Address integrity concerns directly
- Connect to institutional policies
